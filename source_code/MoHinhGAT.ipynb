{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff0c1626",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GATConv\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "# 1. ĐỌC & TIỀN XỬ LÝ DỮ LIỆU\n",
    "df = pd.read_csv(\"f_coco_triplets.csv\")\n",
    "\n",
    "# Gán chỉ số cho entity\n",
    "all_entities = pd.concat([df['subject'], df['object']]).unique()\n",
    "entity_encoder = LabelEncoder()\n",
    "entity_encoder.fit(all_entities)\n",
    "\n",
    "df['subject_id'] = entity_encoder.transform(df['subject'])\n",
    "df['object_id'] = entity_encoder.transform(df['object'])\n",
    "\n",
    "# Gán chỉ số cho quan hệ\n",
    "relation_encoder = LabelEncoder()\n",
    "df['predicate_id'] = relation_encoder.fit_transform(df['predicate'])\n",
    "\n",
    "# Node features: one-hot cho mỗi entity\n",
    "num_nodes = len(entity_encoder.classes_)\n",
    "x = torch.eye(num_nodes)\n",
    "\n",
    "# Edge index\n",
    "edge_index = torch.tensor([df['subject_id'].values, df['object_id'].values], dtype=torch.long)\n",
    "data = Data(x=x, edge_index=edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2cacb0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. MÔ HÌNH GAT\n",
    "class GATModel(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.gat1 = GATConv(in_channels, hidden_channels, heads=4)\n",
    "        self.gat2 = GATConv(hidden_channels * 4, out_channels, heads=1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.gat1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.gat2(x, edge_index)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b87e60f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. CONTRASTIVE LOSS\n",
    "def contrastive_loss(x_i, x_j, label, margin=1.0):\n",
    "    dist = F.pairwise_distance(x_i, x_j)\n",
    "    return (label * dist.pow(2) + (1 - label) * F.relu(margin - dist).pow(2)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "721f6f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. HUẤN LUYỆN MÔ HÌNH\n",
    "def train_model(data, model, epochs=100, lr=0.005, samples_per_epoch=2000):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    edge_index = data.edge_index.t().tolist()\n",
    "    all_nodes = list(range(data.num_nodes))\n",
    "    existing_edges = set(tuple(e) for e in edge_index)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        embeddings = model(data)\n",
    "\n",
    "        # Positive samples: từ các cạnh trong đồ thị\n",
    "        pos_samples = random.sample(edge_index, min(samples_per_epoch, len(edge_index)))\n",
    "\n",
    "        # Negative samples: node pair không kết nối\n",
    "        neg_samples = []\n",
    "        while len(neg_samples) < len(pos_samples):\n",
    "            i, j = random.sample(all_nodes, 2)\n",
    "            if (i, j) not in existing_edges and (j, i) not in existing_edges:\n",
    "                neg_samples.append((i, j))\n",
    "\n",
    "        # Lấy embedding\n",
    "        pos_i = torch.stack([embeddings[i] for i, j in pos_samples])\n",
    "        pos_j = torch.stack([embeddings[j] for i, j in pos_samples])\n",
    "        neg_i = torch.stack([embeddings[i] for i, j in neg_samples])\n",
    "        neg_j = torch.stack([embeddings[j] for i, j in neg_samples])\n",
    "\n",
    "        # Tính loss\n",
    "        loss_pos = contrastive_loss(pos_i, pos_j, torch.ones(len(pos_i)))\n",
    "        loss_neg = contrastive_loss(neg_i, neg_j, torch.zeros(len(neg_i)))\n",
    "        loss = loss_pos + loss_neg\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch}: Loss = {loss.item():.4f}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15f467d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 0.6871\n",
      "Epoch 1: Loss = 0.5856\n",
      "Epoch 2: Loss = 0.4865\n",
      "Epoch 3: Loss = 0.4020\n",
      "Epoch 4: Loss = 0.3462\n",
      "Epoch 5: Loss = 0.3184\n",
      "Epoch 6: Loss = 0.2966\n",
      "Epoch 7: Loss = 0.2827\n",
      "Epoch 8: Loss = 0.2786\n",
      "Epoch 9: Loss = 0.2631\n",
      "Epoch 10: Loss = 0.2464\n",
      "Epoch 11: Loss = 0.2342\n",
      "Epoch 12: Loss = 0.2285\n",
      "Epoch 13: Loss = 0.2116\n",
      "Epoch 14: Loss = 0.2058\n",
      "Epoch 15: Loss = 0.2068\n",
      "Epoch 16: Loss = 0.2002\n",
      "Epoch 17: Loss = 0.2042\n",
      "Epoch 18: Loss = 0.1912\n",
      "Epoch 19: Loss = 0.1898\n",
      "Epoch 20: Loss = 0.1936\n",
      "Epoch 21: Loss = 0.1842\n",
      "Epoch 22: Loss = 0.1827\n",
      "Epoch 23: Loss = 0.1794\n",
      "Epoch 24: Loss = 0.1790\n",
      "Epoch 25: Loss = 0.1743\n",
      "Epoch 26: Loss = 0.1745\n",
      "Epoch 27: Loss = 0.1718\n",
      "Epoch 28: Loss = 0.1724\n",
      "Epoch 29: Loss = 0.1707\n",
      "Epoch 30: Loss = 0.1697\n",
      "Epoch 31: Loss = 0.1656\n",
      "Epoch 32: Loss = 0.1656\n",
      "Epoch 33: Loss = 0.1665\n",
      "Epoch 34: Loss = 0.1652\n",
      "Epoch 35: Loss = 0.1631\n",
      "Epoch 36: Loss = 0.1583\n",
      "Epoch 37: Loss = 0.1586\n",
      "Epoch 38: Loss = 0.1635\n",
      "Epoch 39: Loss = 0.1584\n",
      "Epoch 40: Loss = 0.1552\n",
      "Epoch 41: Loss = 0.1586\n",
      "Epoch 42: Loss = 0.1586\n",
      "Epoch 43: Loss = 0.1517\n",
      "Epoch 44: Loss = 0.1547\n",
      "Epoch 45: Loss = 0.1576\n",
      "Epoch 46: Loss = 0.1543\n",
      "Epoch 47: Loss = 0.1503\n",
      "Epoch 48: Loss = 0.1518\n",
      "Epoch 49: Loss = 0.1510\n",
      "Epoch 50: Loss = 0.1538\n",
      "Epoch 51: Loss = 0.1538\n",
      "Epoch 52: Loss = 0.1474\n",
      "Epoch 53: Loss = 0.1508\n",
      "Epoch 54: Loss = 0.1536\n",
      "Epoch 55: Loss = 0.1504\n",
      "Epoch 56: Loss = 0.1549\n",
      "Epoch 57: Loss = 0.1515\n",
      "Epoch 58: Loss = 0.1481\n",
      "Epoch 59: Loss = 0.1451\n",
      "Epoch 60: Loss = 0.1495\n",
      "Epoch 61: Loss = 0.1521\n",
      "Epoch 62: Loss = 0.1504\n",
      "Epoch 63: Loss = 0.1511\n",
      "Epoch 64: Loss = 0.1479\n",
      "Epoch 65: Loss = 0.1503\n",
      "Epoch 66: Loss = 0.1457\n",
      "Epoch 67: Loss = 0.1483\n",
      "Epoch 68: Loss = 0.1512\n",
      "Epoch 69: Loss = 0.1530\n",
      "Epoch 70: Loss = 0.1454\n",
      "Epoch 71: Loss = 0.1467\n",
      "Epoch 72: Loss = 0.1494\n",
      "Epoch 73: Loss = 0.1474\n",
      "Epoch 74: Loss = 0.1482\n",
      "Epoch 75: Loss = 0.1458\n",
      "Epoch 76: Loss = 0.1505\n",
      "Epoch 77: Loss = 0.1482\n",
      "Epoch 78: Loss = 0.1438\n",
      "Epoch 79: Loss = 0.1525\n",
      "Epoch 80: Loss = 0.1446\n",
      "Epoch 81: Loss = 0.1493\n",
      "Epoch 82: Loss = 0.1413\n",
      "Epoch 83: Loss = 0.1455\n",
      "Epoch 84: Loss = 0.1454\n",
      "Epoch 85: Loss = 0.1483\n",
      "Epoch 86: Loss = 0.1441\n",
      "Epoch 87: Loss = 0.1441\n",
      "Epoch 88: Loss = 0.1434\n",
      "Epoch 89: Loss = 0.1439\n",
      "Epoch 90: Loss = 0.1486\n",
      "Epoch 91: Loss = 0.1517\n",
      "Epoch 92: Loss = 0.1444\n",
      "Epoch 93: Loss = 0.1484\n",
      "Epoch 94: Loss = 0.1418\n",
      "Epoch 95: Loss = 0.1465\n",
      "Epoch 96: Loss = 0.1521\n",
      "Epoch 97: Loss = 0.1452\n",
      "Epoch 98: Loss = 0.1407\n",
      "Epoch 99: Loss = 0.1472\n"
     ]
    }
   ],
   "source": [
    "# 5. CHẠY HUẤN LUYỆN\n",
    "model = GATModel(in_channels=x.shape[1], hidden_channels=32, out_channels=64)\n",
    "trained_model = train_model(data, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bca077b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. LẤY EMBEDDING CHO CÁC NODE\n",
    "node_embeddings = trained_model(data).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8d78e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. TẠO BẢN ĐỒ ENTITY -> IMAGE\n",
    "entity_idx_to_images = defaultdict(set)\n",
    "for i, row in df.iterrows():\n",
    "    entity_idx_to_images[row['subject_id']].add(row['image_id'])\n",
    "    entity_idx_to_images[row['object_id']].add(row['image_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "254a7317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tất cả file đã được lưu vào thư mục 'saved_model_GAT/'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "# === 8. LƯU FILE PHỤC VỤ STREAMLIT / API ===\n",
    "output_dir = \"saved_model_GAT\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 8.1 Lưu LabelEncoder cho entity\n",
    "with open(os.path.join(output_dir, \"entity_encoder.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(entity_encoder, f)\n",
    "\n",
    "# 8.2 Lưu LabelEncoder cho predicate (nếu cần)\n",
    "with open(os.path.join(output_dir, \"relation_encoder.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(relation_encoder, f)\n",
    "\n",
    "# 8.3 Lưu node embeddings\n",
    "np.save(os.path.join(output_dir, \"node_embeddings.npy\"), node_embeddings)\n",
    "\n",
    "# 8.4 Lưu trained model (chỉ weights)\n",
    "torch.save(trained_model.state_dict(), os.path.join(output_dir, \"gat_model_weights.pt\"))\n",
    "\n",
    "# 8.5 Lưu entity-to-image mapping\n",
    "with open(os.path.join(output_dir, \"entity_idx_to_images.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(entity_idx_to_images, f)\n",
    "\n",
    "print(\"Tất cả file đã được lưu vào thư mục 'saved_model_GAT/'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f091f645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. HÀM TÌM ẢNH LIÊN QUAN\n",
    "def find_related_images(query_image_id, top_k=5):\n",
    "    related_entities = set(df[df['image_id'] == query_image_id]['subject_id']) | \\\n",
    "                       set(df[df['image_id'] == query_image_id]['object_id'])\n",
    "\n",
    "    if not related_entities:\n",
    "        return []\n",
    "\n",
    "    query_vec = sum(torch.tensor(node_embeddings[i]) for i in related_entities) / len(related_entities)\n",
    "    scores = cosine_similarity(query_vec.reshape(1, -1), node_embeddings)[0]\n",
    "\n",
    "    top_entity_indices = scores.argsort()[-top_k:][::-1]\n",
    "\n",
    "    related_images = set()\n",
    "    for idx in top_entity_indices:\n",
    "        related_images.update(entity_idx_to_images[idx])\n",
    "\n",
    "    return list(related_images - {query_image_id})[:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "287ea5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. HÀM ĐÁNH GIÁ PRECISION / RECALL / F1\n",
    "def evaluate_retrieval(df, node_embeddings, entity_idx_to_images, top_k=5, sample_size=100):\n",
    "    image_ids = list(df['image_id'].unique())\n",
    "    sampled_queries = random.sample(image_ids, min(sample_size, len(image_ids)))\n",
    "\n",
    "    precision_list, recall_list, f1_list = [], [], []\n",
    "\n",
    "    for query_id in sampled_queries:\n",
    "        related_entities = set(df[df['image_id'] == query_id]['subject_id']) | \\\n",
    "                           set(df[df['image_id'] == query_id]['object_id'])\n",
    "\n",
    "        ground_truth = set()\n",
    "        for e in related_entities:\n",
    "            ground_truth.update(entity_idx_to_images[e])\n",
    "        ground_truth.discard(query_id)\n",
    "\n",
    "        predicted = set(find_related_images(query_id, top_k=top_k))\n",
    "\n",
    "        tp = len(predicted & ground_truth)\n",
    "        fp = len(predicted - ground_truth)\n",
    "        fn = len(ground_truth - predicted)\n",
    "\n",
    "        precision = tp / (tp + fp + 1e-6)\n",
    "        recall = tp / (tp + fn + 1e-6)\n",
    "        f1 = 2 * precision * recall / (precision + recall + 1e-6)\n",
    "\n",
    "        precision_list.append(precision)\n",
    "        recall_list.append(recall)\n",
    "        f1_list.append(f1)\n",
    "\n",
    "    return {\n",
    "        \"Precision@{}\".format(top_k): round(sum(precision_list) / len(precision_list), 4),\n",
    "        \"Recall@{}\".format(top_k): round(sum(recall_list) / len(recall_list), 4),\n",
    "        \"F1-score@{}\".format(top_k): round(sum(f1_list) / len(f1_list), 4)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e1930800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kết quả đánh giá:\n",
      "{'Precision@5': 0.6163, 'Recall@5': 0.1418, 'F1-score@5': 0.1503}\n"
     ]
    }
   ],
   "source": [
    "# 11. CHẠY ĐÁNH GIÁ\n",
    "results = evaluate_retrieval(df, node_embeddings, entity_idx_to_images, top_k=5, sample_size=100)\n",
    "print(\"Kết quả đánh giá:\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75866bf2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
