{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fb54809",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef11521c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TransH Model Definition\n",
    "# ------------------------\n",
    "class TransH(nn.Module):\n",
    "    def __init__(self, num_entities, num_relations, embedding_dim=100, margin=1.0):\n",
    "        super(TransH, self).__init__()\n",
    "        self.margin = margin\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        self.ent_embeddings = nn.Embedding(num_entities, embedding_dim)\n",
    "        self.rel_embeddings = nn.Embedding(num_relations, embedding_dim)\n",
    "        self.norm_embeddings = nn.Embedding(num_relations, embedding_dim)  # vector pháp tuyến\n",
    "\n",
    "        nn.init.xavier_uniform_(self.ent_embeddings.weight.data)\n",
    "        nn.init.xavier_uniform_(self.rel_embeddings.weight.data)\n",
    "        nn.init.xavier_uniform_(self.norm_embeddings.weight.data)\n",
    "\n",
    "    def _project(self, e, norm):\n",
    "        norm = F.normalize(norm, p=2, dim=-1)\n",
    "        return e - torch.sum(e * norm, dim=-1, keepdim=True) * norm\n",
    "\n",
    "    def forward(self, pos_triples, neg_triples):\n",
    "        ph = self.ent_embeddings(pos_triples[:, 0])\n",
    "        pr = self.rel_embeddings(pos_triples[:, 1])\n",
    "        pt = self.ent_embeddings(pos_triples[:, 2])\n",
    "        pn = self.norm_embeddings(pos_triples[:, 1])\n",
    "\n",
    "        nh = self.ent_embeddings(neg_triples[:, 0])\n",
    "        nr = self.rel_embeddings(neg_triples[:, 1])\n",
    "        nt = self.ent_embeddings(neg_triples[:, 2])\n",
    "        nn_ = self.norm_embeddings(neg_triples[:, 1])\n",
    "\n",
    "        ph_proj = self._project(ph, pn)\n",
    "        pt_proj = self._project(pt, pn)\n",
    "        nh_proj = self._project(nh, nn_)\n",
    "        nt_proj = self._project(nt, nn_)\n",
    "\n",
    "        pos_score = torch.norm(ph_proj + pr - pt_proj, p=2, dim=1)\n",
    "        neg_score = torch.norm(nh_proj + nr - nt_proj, p=2, dim=1)\n",
    "\n",
    "        return pos_score, neg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac72e382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negative Sampling Helper\n",
    "# ------------------------\n",
    "def corrupt_batch(batch, num_entities):\n",
    "    corrupted = batch.copy()\n",
    "    for i in range(len(batch)):\n",
    "        if np.random.rand() < 0.5:\n",
    "            corrupted[i][0] = np.random.randint(0, num_entities)\n",
    "        else:\n",
    "            corrupted[i][2] = np.random.randint(0, num_entities)\n",
    "    return corrupted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74841a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Function\n",
    "# ------------------------\n",
    "def train_model(model, train_data, num_entities, num_relations, num_epochs=100, batch_size=512, lr=0.001, patience=10):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MarginRankingLoss(margin=model.margin)\n",
    "    best_loss = float('inf')\n",
    "    wait = 0\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        np.random.shuffle(train_data)\n",
    "\n",
    "        for i in range(0, len(train_data), batch_size):\n",
    "            batch_pos = train_data[i:i+batch_size]\n",
    "            batch_neg = corrupt_batch(batch_pos.copy(), num_entities)\n",
    "\n",
    "            batch_pos = torch.tensor(batch_pos, dtype=torch.long)\n",
    "            batch_neg = torch.tensor(batch_neg, dtype=torch.long)\n",
    "            y = torch.ones(batch_pos.size(0))\n",
    "\n",
    "            pos_score, neg_score = model(batch_pos, batch_neg)\n",
    "            loss = criterion(pos_score, neg_score, y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / (len(train_data) / batch_size)\n",
    "        print(f\"Epoch {epoch+1} | Avg Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # Early stopping\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            wait = 0\n",
    "            torch.save(model.state_dict(), \"best_transh_model.pth\")\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1} (best loss: {best_loss:.4f})\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47143705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ví dụ: DataFrame chứa triplets\n",
    "df_triplet = pd.read_csv(\"f_coco_triplets.csv\")\n",
    "entities = pd.concat([df_triplet[\"subject\"], df_triplet[\"object\"]]).unique()\n",
    "entity2id = {entity: idx for idx, entity in enumerate(entities)}\n",
    "relation2id = {rel: idx for idx, rel in enumerate(df_triplet[\"predicate\"].unique())}\n",
    "\n",
    "df_triplet[\"head_id\"] = df_triplet[\"subject\"].map(entity2id)\n",
    "df_triplet[\"tail_id\"] = df_triplet[\"object\"].map(entity2id)\n",
    "df_triplet[\"relation_id\"] = df_triplet[\"predicate\"].map(relation2id)\n",
    "\n",
    "triplets = df_triplet[[\"head_id\", \"relation_id\", \"tail_id\"]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "307b6dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Avg Loss: 1.0728\n",
      "Epoch 2 | Avg Loss: 1.0439\n",
      "Epoch 3 | Avg Loss: 1.0144\n",
      "Epoch 4 | Avg Loss: 0.9841\n",
      "Epoch 5 | Avg Loss: 0.9533\n",
      "Epoch 6 | Avg Loss: 0.9201\n",
      "Epoch 7 | Avg Loss: 0.8886\n",
      "Epoch 8 | Avg Loss: 0.8548\n",
      "Epoch 9 | Avg Loss: 0.8242\n",
      "Epoch 10 | Avg Loss: 0.7881\n",
      "Epoch 11 | Avg Loss: 0.7539\n",
      "Epoch 12 | Avg Loss: 0.7160\n",
      "Epoch 13 | Avg Loss: 0.6819\n",
      "Epoch 14 | Avg Loss: 0.6562\n",
      "Epoch 15 | Avg Loss: 0.6266\n",
      "Epoch 16 | Avg Loss: 0.5988\n",
      "Epoch 17 | Avg Loss: 0.5767\n",
      "Epoch 18 | Avg Loss: 0.5497\n",
      "Epoch 19 | Avg Loss: 0.5221\n",
      "Epoch 20 | Avg Loss: 0.5062\n",
      "Epoch 21 | Avg Loss: 0.4836\n",
      "Epoch 22 | Avg Loss: 0.4744\n",
      "Epoch 23 | Avg Loss: 0.4638\n",
      "Epoch 24 | Avg Loss: 0.4559\n",
      "Epoch 25 | Avg Loss: 0.4421\n",
      "Epoch 26 | Avg Loss: 0.4330\n",
      "Epoch 27 | Avg Loss: 0.4272\n",
      "Epoch 28 | Avg Loss: 0.4176\n",
      "Epoch 29 | Avg Loss: 0.4015\n",
      "Epoch 30 | Avg Loss: 0.3864\n",
      "Epoch 31 | Avg Loss: 0.3894\n",
      "Epoch 32 | Avg Loss: 0.3795\n",
      "Epoch 33 | Avg Loss: 0.3721\n",
      "Epoch 34 | Avg Loss: 0.3769\n",
      "Epoch 35 | Avg Loss: 0.3616\n",
      "Epoch 36 | Avg Loss: 0.3679\n",
      "Epoch 37 | Avg Loss: 0.3491\n",
      "Epoch 38 | Avg Loss: 0.3477\n",
      "Epoch 39 | Avg Loss: 0.3409\n",
      "Epoch 40 | Avg Loss: 0.3476\n",
      "Epoch 41 | Avg Loss: 0.3333\n",
      "Epoch 42 | Avg Loss: 0.3287\n",
      "Epoch 43 | Avg Loss: 0.3135\n",
      "Epoch 44 | Avg Loss: 0.3180\n",
      "Epoch 45 | Avg Loss: 0.3062\n",
      "Epoch 46 | Avg Loss: 0.3039\n",
      "Epoch 47 | Avg Loss: 0.3107\n",
      "Epoch 48 | Avg Loss: 0.3019\n",
      "Epoch 49 | Avg Loss: 0.2939\n",
      "Epoch 50 | Avg Loss: 0.2945\n",
      "Epoch 51 | Avg Loss: 0.2970\n",
      "Epoch 52 | Avg Loss: 0.2897\n",
      "Epoch 53 | Avg Loss: 0.2800\n",
      "Epoch 54 | Avg Loss: 0.2732\n",
      "Epoch 55 | Avg Loss: 0.2786\n",
      "Epoch 56 | Avg Loss: 0.2760\n",
      "Epoch 57 | Avg Loss: 0.2846\n",
      "Epoch 58 | Avg Loss: 0.2816\n",
      "Epoch 59 | Avg Loss: 0.2679\n",
      "Epoch 60 | Avg Loss: 0.2745\n",
      "Epoch 61 | Avg Loss: 0.2690\n",
      "Epoch 62 | Avg Loss: 0.2536\n",
      "Epoch 63 | Avg Loss: 0.2629\n",
      "Epoch 64 | Avg Loss: 0.2564\n",
      "Epoch 65 | Avg Loss: 0.2488\n",
      "Epoch 66 | Avg Loss: 0.2452\n",
      "Epoch 67 | Avg Loss: 0.2387\n",
      "Epoch 68 | Avg Loss: 0.2378\n",
      "Epoch 69 | Avg Loss: 0.2295\n",
      "Epoch 70 | Avg Loss: 0.2424\n",
      "Epoch 71 | Avg Loss: 0.2363\n",
      "Epoch 72 | Avg Loss: 0.2276\n",
      "Epoch 73 | Avg Loss: 0.2369\n",
      "Epoch 74 | Avg Loss: 0.2271\n",
      "Epoch 75 | Avg Loss: 0.2340\n",
      "Epoch 76 | Avg Loss: 0.2230\n",
      "Epoch 77 | Avg Loss: 0.2290\n",
      "Epoch 78 | Avg Loss: 0.2215\n",
      "Epoch 79 | Avg Loss: 0.2139\n",
      "Epoch 80 | Avg Loss: 0.2166\n",
      "Epoch 81 | Avg Loss: 0.2120\n",
      "Epoch 82 | Avg Loss: 0.2098\n",
      "Epoch 83 | Avg Loss: 0.2151\n",
      "Epoch 84 | Avg Loss: 0.2163\n",
      "Epoch 85 | Avg Loss: 0.2175\n",
      "Epoch 86 | Avg Loss: 0.2061\n",
      "Epoch 87 | Avg Loss: 0.1934\n",
      "Epoch 88 | Avg Loss: 0.1998\n",
      "Epoch 89 | Avg Loss: 0.1954\n",
      "Epoch 90 | Avg Loss: 0.1932\n",
      "Epoch 91 | Avg Loss: 0.1836\n",
      "Epoch 92 | Avg Loss: 0.1950\n",
      "Epoch 93 | Avg Loss: 0.1936\n",
      "Epoch 94 | Avg Loss: 0.1932\n",
      "Epoch 95 | Avg Loss: 0.1865\n",
      "Epoch 96 | Avg Loss: 0.1904\n",
      "Epoch 97 | Avg Loss: 0.1829\n",
      "Epoch 98 | Avg Loss: 0.1763\n",
      "Epoch 99 | Avg Loss: 0.1818\n",
      "Epoch 100 | Avg Loss: 0.1730\n",
      "Epoch 101 | Avg Loss: 0.1794\n",
      "Epoch 102 | Avg Loss: 0.1742\n",
      "Epoch 103 | Avg Loss: 0.1779\n",
      "Epoch 104 | Avg Loss: 0.1640\n",
      "Epoch 105 | Avg Loss: 0.1677\n",
      "Epoch 106 | Avg Loss: 0.1728\n",
      "Epoch 107 | Avg Loss: 0.1735\n",
      "Epoch 108 | Avg Loss: 0.1658\n",
      "Epoch 109 | Avg Loss: 0.1695\n",
      "Epoch 110 | Avg Loss: 0.1638\n",
      "Epoch 111 | Avg Loss: 0.1570\n",
      "Epoch 112 | Avg Loss: 0.1602\n",
      "Epoch 113 | Avg Loss: 0.1613\n",
      "Epoch 114 | Avg Loss: 0.1576\n",
      "Epoch 115 | Avg Loss: 0.1546\n",
      "Epoch 116 | Avg Loss: 0.1563\n",
      "Epoch 117 | Avg Loss: 0.1557\n",
      "Epoch 118 | Avg Loss: 0.1508\n",
      "Epoch 119 | Avg Loss: 0.1523\n",
      "Epoch 120 | Avg Loss: 0.1520\n",
      "Epoch 121 | Avg Loss: 0.1530\n",
      "Epoch 122 | Avg Loss: 0.1514\n",
      "Epoch 123 | Avg Loss: 0.1489\n",
      "Epoch 124 | Avg Loss: 0.1536\n",
      "Epoch 125 | Avg Loss: 0.1492\n",
      "Epoch 126 | Avg Loss: 0.1465\n",
      "Epoch 127 | Avg Loss: 0.1447\n",
      "Epoch 128 | Avg Loss: 0.1371\n",
      "Epoch 129 | Avg Loss: 0.1396\n",
      "Epoch 130 | Avg Loss: 0.1368\n",
      "Epoch 131 | Avg Loss: 0.1355\n",
      "Epoch 132 | Avg Loss: 0.1452\n",
      "Epoch 133 | Avg Loss: 0.1304\n",
      "Epoch 134 | Avg Loss: 0.1341\n",
      "Epoch 135 | Avg Loss: 0.1297\n",
      "Epoch 136 | Avg Loss: 0.1329\n",
      "Epoch 137 | Avg Loss: 0.1388\n",
      "Epoch 138 | Avg Loss: 0.1376\n",
      "Epoch 139 | Avg Loss: 0.1265\n",
      "Epoch 140 | Avg Loss: 0.1211\n",
      "Epoch 141 | Avg Loss: 0.1332\n",
      "Epoch 142 | Avg Loss: 0.1301\n",
      "Epoch 143 | Avg Loss: 0.1207\n",
      "Epoch 144 | Avg Loss: 0.1255\n",
      "Epoch 145 | Avg Loss: 0.1268\n",
      "Epoch 146 | Avg Loss: 0.1236\n",
      "Epoch 147 | Avg Loss: 0.1253\n",
      "Epoch 148 | Avg Loss: 0.1230\n",
      "Epoch 149 | Avg Loss: 0.1248\n",
      "Epoch 150 | Avg Loss: 0.1181\n",
      "Epoch 151 | Avg Loss: 0.1224\n",
      "Epoch 152 | Avg Loss: 0.1209\n",
      "Epoch 153 | Avg Loss: 0.1187\n",
      "Epoch 154 | Avg Loss: 0.1241\n",
      "Epoch 155 | Avg Loss: 0.1185\n",
      "Epoch 156 | Avg Loss: 0.1125\n",
      "Epoch 157 | Avg Loss: 0.1135\n",
      "Epoch 158 | Avg Loss: 0.1136\n",
      "Epoch 159 | Avg Loss: 0.1099\n",
      "Epoch 160 | Avg Loss: 0.1097\n",
      "Epoch 161 | Avg Loss: 0.1090\n",
      "Epoch 162 | Avg Loss: 0.1070\n",
      "Epoch 163 | Avg Loss: 0.1043\n",
      "Epoch 164 | Avg Loss: 0.1118\n",
      "Epoch 165 | Avg Loss: 0.1048\n",
      "Epoch 166 | Avg Loss: 0.1062\n",
      "Epoch 167 | Avg Loss: 0.1044\n",
      "Epoch 168 | Avg Loss: 0.1036\n",
      "Epoch 169 | Avg Loss: 0.1024\n",
      "Epoch 170 | Avg Loss: 0.1051\n",
      "Epoch 171 | Avg Loss: 0.1087\n",
      "Epoch 172 | Avg Loss: 0.0982\n",
      "Epoch 173 | Avg Loss: 0.1028\n",
      "Epoch 174 | Avg Loss: 0.0978\n",
      "Epoch 175 | Avg Loss: 0.1006\n",
      "Epoch 176 | Avg Loss: 0.1050\n",
      "Epoch 177 | Avg Loss: 0.0999\n",
      "Epoch 178 | Avg Loss: 0.0969\n",
      "Epoch 179 | Avg Loss: 0.0990\n",
      "Epoch 180 | Avg Loss: 0.0978\n",
      "Epoch 181 | Avg Loss: 0.1030\n",
      "Epoch 182 | Avg Loss: 0.0988\n",
      "Epoch 183 | Avg Loss: 0.0935\n",
      "Epoch 184 | Avg Loss: 0.0981\n",
      "Epoch 185 | Avg Loss: 0.0995\n",
      "Epoch 186 | Avg Loss: 0.0862\n",
      "Epoch 187 | Avg Loss: 0.0912\n",
      "Epoch 188 | Avg Loss: 0.0904\n",
      "Epoch 189 | Avg Loss: 0.0924\n",
      "Epoch 190 | Avg Loss: 0.0924\n",
      "Epoch 191 | Avg Loss: 0.0865\n",
      "Epoch 192 | Avg Loss: 0.0886\n",
      "Epoch 193 | Avg Loss: 0.0934\n",
      "Epoch 194 | Avg Loss: 0.0933\n",
      "Epoch 195 | Avg Loss: 0.0934\n",
      "Epoch 196 | Avg Loss: 0.0853\n",
      "Epoch 197 | Avg Loss: 0.0957\n",
      "Epoch 198 | Avg Loss: 0.0867\n",
      "Epoch 199 | Avg Loss: 0.0833\n",
      "Epoch 200 | Avg Loss: 0.0859\n",
      "Epoch 201 | Avg Loss: 0.0870\n",
      "Epoch 202 | Avg Loss: 0.0824\n",
      "Epoch 203 | Avg Loss: 0.0829\n",
      "Epoch 204 | Avg Loss: 0.0797\n",
      "Epoch 205 | Avg Loss: 0.0814\n",
      "Epoch 206 | Avg Loss: 0.0901\n",
      "Epoch 207 | Avg Loss: 0.0876\n",
      "Epoch 208 | Avg Loss: 0.0772\n",
      "Epoch 209 | Avg Loss: 0.0763\n",
      "Epoch 210 | Avg Loss: 0.0807\n",
      "Epoch 211 | Avg Loss: 0.0813\n",
      "Epoch 212 | Avg Loss: 0.0816\n",
      "Epoch 213 | Avg Loss: 0.0903\n",
      "Epoch 214 | Avg Loss: 0.0849\n",
      "Epoch 215 | Avg Loss: 0.0818\n",
      "Epoch 216 | Avg Loss: 0.0765\n",
      "Epoch 217 | Avg Loss: 0.0779\n",
      "Epoch 218 | Avg Loss: 0.0796\n",
      "Epoch 219 | Avg Loss: 0.0803\n",
      "Early stopping at epoch 219 (best loss: 0.0763)\n"
     ]
    }
   ],
   "source": [
    "model = TransH(num_entities=len(entity2id), num_relations=len(relation2id), embedding_dim=100, margin=1.0)\n",
    "train_model(model, triplets, len(entity2id), len(relation2id), num_epochs=300, batch_size=512, lr=0.001, patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1cd197fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding Saving\n",
    "# ------------------------\n",
    "def save_embeddings(model, entity2id, relation2id):\n",
    "    entity_embeddings = model.ent_embeddings.weight.detach().cpu().numpy()\n",
    "    relation_embeddings = model.rel_embeddings.weight.detach().cpu().numpy()\n",
    "\n",
    "    id2entity = {v: k for k, v in entity2id.items()}\n",
    "    id2relation = {v: k for k, v in relation2id.items()}\n",
    "\n",
    "    entity_df = pd.DataFrame(entity_embeddings)\n",
    "    entity_df.insert(0, \"entity\", [id2entity[i] for i in range(len(id2entity))])\n",
    "    entity_df.to_csv(\"entity_embeddings_transh.csv\", index=False)\n",
    "\n",
    "    relation_df = pd.DataFrame(relation_embeddings)\n",
    "    relation_df.insert(0, \"relation\", [id2relation[i] for i in range(len(id2relation))])\n",
    "    relation_df.to_csv(\"relation_embeddings_transh.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d3c00df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Hàm đánh giá 1 triplet\n",
    "def evaluate_single_triplet_transH(head_id, rel_id, tail_id, entity_embeddings, relation_embeddings, image_embeddings, image_ids, img_entity_map, top_k=5):\n",
    "    # Truy xuất vector embedding\n",
    "    h_emb = entity_embeddings[head_id]\n",
    "    r_emb = relation_embeddings[rel_id]\n",
    "    t_emb = entity_embeddings[tail_id]\n",
    "\n",
    "    # Vector dự đoán: h + r\n",
    "    pred_vec = h_emb + r_emb\n",
    "\n",
    "    # Tính khoảng cách tới các entity của từng ảnh\n",
    "    scores = []\n",
    "    for img_idx, img_id in enumerate(image_ids):\n",
    "        entities_in_image = img_entity_map.get(img_id, set())\n",
    "        if not entities_in_image:\n",
    "            continue\n",
    "        dists = [np.linalg.norm(pred_vec - entity_embeddings[e_id]) for e_id in entities_in_image]\n",
    "        score = min(dists)  # Lấy khoảng cách ngắn nhất\n",
    "        scores.append((img_id, score))\n",
    "\n",
    "    # Lấy top-K ảnh gần nhất (khoảng cách nhỏ nhất)\n",
    "    scores = sorted(scores, key=lambda x: x[1])[:top_k]\n",
    "    predicted_ids = set(str(x[0]) for x in scores)\n",
    "    \n",
    "    return predicted_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aa82b57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_transH_model(entity_embeddings, relation_embeddings, image_embeddings, image_ids, test_triplets, entity2id, relation2id, img_entity_map, all_map, top_k=5):\n",
    "    gt_dict = defaultdict(set)\n",
    "    for _, row in all_map.iterrows():\n",
    "        key = (row[\"subject\"], row[\"predicate\"], row[\"object\"])\n",
    "        gt_dict[key].add(str(row[\"image_id\"]))\n",
    "\n",
    "    total_p, total_r, total_f1 = 0, 0, 0\n",
    "    count = 0\n",
    "\n",
    "    for _, row in tqdm(test_triplets.iterrows(), total=len(test_triplets)):\n",
    "        query_key = (row[\"subject\"], row[\"predicate\"], row[\"object\"])\n",
    "        h = entity2id.get(row[\"subject\"])\n",
    "        r = relation2id.get(row[\"predicate\"])\n",
    "        t = entity2id.get(row[\"object\"])\n",
    "        if h is None or r is None or t is None:\n",
    "            continue\n",
    "\n",
    "        pred_ids = evaluate_single_triplet_transH(h, r, t, entity_embeddings, relation_embeddings, image_embeddings, image_ids, img_entity_map, top_k)\n",
    "        true_ids = gt_dict.get(query_key, set())\n",
    "\n",
    "        if not true_ids:\n",
    "            continue\n",
    "\n",
    "        tp = len(pred_ids & true_ids)\n",
    "        fp = len(pred_ids - true_ids)\n",
    "        fn = len(true_ids - pred_ids)\n",
    "\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall    = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1        = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "        total_p += precision\n",
    "        total_r += recall\n",
    "        total_f1 += f1\n",
    "        count += 1\n",
    "\n",
    "    if count > 0:\n",
    "        print(f\"\\nEvaluation Results (top-{top_k}):\")\n",
    "        print(f\"→ Precision@{top_k}: {total_p/count:.4f}\")\n",
    "        print(f\"→ Recall@{top_k}:    {total_r/count:.4f}\")\n",
    "        print(f\"→ F1-score@{top_k}:  {total_f1/count:.4f}\")\n",
    "    else:\n",
    "        print(\"No valid triplets for evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aef999c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ảnh và feature (đã trích xuất từ mô hình CNN)\n",
    "image_embeddings = np.load(\"image_features.npy\")  # shape (num_images, d)\n",
    "image_ids = np.load(\"image_ids.npy\")  # danh sách ID ảnh, dạng str hoặc int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bb616824",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Load triplet mapping\n",
    "df_all = pd.read_csv(\"f_coco_triplets.csv\")  # phải có cột: subject, predicate, object, image_id\n",
    "\n",
    "# Ánh xạ entity\n",
    "entities = pd.concat([df_all[\"subject\"], df_all[\"object\"]]).unique()\n",
    "entity2id = {entity: idx for idx, entity in enumerate(entities)}\n",
    "relation2id = {rel: idx for idx, rel in enumerate(df_all[\"predicate\"].unique())}\n",
    "\n",
    "# Tạo ánh xạ image_id → tập entity_id\n",
    "img_entity_map = defaultdict(set)\n",
    "for _, row in df_all.iterrows():\n",
    "    img_id = row[\"image_id\"]\n",
    "    subj = row[\"subject\"]\n",
    "    obj = row[\"object\"]\n",
    "    if subj in entity2id:\n",
    "        img_entity_map[str(img_id)].add(entity2id[subj])\n",
    "    if obj in entity2id:\n",
    "        img_entity_map[str(img_id)].add(entity2id[obj])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "745c0e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lưu embedding sau khi huấn luyện\n",
    "save_embeddings(model, entity2id, relation2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e2729baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nếu bạn vừa train model:\n",
    "entity_embeddings = model.ent_embeddings.weight.detach().cpu().numpy()\n",
    "relation_embeddings = model.rel_embeddings.weight.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "975e5681",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_df = pd.read_csv(\"entity_embeddings_transh.csv\")\n",
    "relation_df = pd.read_csv(\"relation_embeddings_transh.csv\")\n",
    "\n",
    "# Tách ID và embedding\n",
    "entity_embeddings = entity_df.drop(\"entity\", axis=1).values\n",
    "relation_embeddings = relation_df.drop(\"relation\", axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "019b6029",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 320/320 [00:00<00:00, 4609.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Results (top-5):\n",
      "→ Precision@5: 0.0000\n",
      "→ Recall@5:    0.0000\n",
      "→ F1-score@5:  0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Chia tập test từ file gốc\n",
    "df_test = df_all.drop_duplicates(subset=[\"subject\", \"predicate\", \"object\", \"image_id\"])\n",
    "test_triplets = df_test.sample(frac=0.1, random_state=42)  # 10% để test\n",
    "\n",
    "# Chạy đánh giá\n",
    "evaluate_transH_model(\n",
    "    entity_embeddings=entity_embeddings,\n",
    "    relation_embeddings=relation_embeddings,\n",
    "    image_embeddings=image_embeddings,\n",
    "    image_ids=[str(i) for i in image_ids],  # ép sang str nếu chưa phải str\n",
    "    test_triplets=test_triplets,\n",
    "    entity2id=entity2id,\n",
    "    relation2id=relation2id,\n",
    "    img_entity_map=img_entity_map,\n",
    "    all_map=df_all,\n",
    "    top_k=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cc47ca01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 663/663 [00:00<00:00, 3500.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Results (top-5):\n",
      "→ Precision@5: 0.0000\n",
      "→ Recall@5:    0.0000\n",
      "→ F1-score@5:  0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load embedding từ file CSV đã lưu\n",
    "def load_embeddings(entity_path, relation_path):\n",
    "    entity_df = pd.read_csv(entity_path)\n",
    "    relation_df = pd.read_csv(relation_path)\n",
    "\n",
    "    entity_names = entity_df[\"entity\"].tolist()\n",
    "    entity_vectors = entity_df.drop(columns=[\"entity\"]).values\n",
    "\n",
    "    relation_names = relation_df[\"relation\"].tolist()\n",
    "    relation_vectors = relation_df.drop(columns=[\"relation\"]).values\n",
    "\n",
    "    entity2id = {name: i for i, name in enumerate(entity_names)}\n",
    "    relation2id = {name: i for i, name in enumerate(relation_names)}\n",
    "\n",
    "    return entity_vectors, relation_vectors, entity2id, relation2id\n",
    "\n",
    "# Load ảnh và embedding ảnh từ CNN\n",
    "def load_image_embeddings(img_feat_path, img_id_path):\n",
    "    image_embeddings = np.load(img_feat_path)\n",
    "    image_ids = np.load(img_id_path)\n",
    "    image_ids = [str(i) for i in image_ids]\n",
    "    return image_embeddings, image_ids\n",
    "\n",
    "# Tạo ánh xạ ảnh → entity\n",
    "def build_img_entity_map(df_all, entity2id):\n",
    "    img_entity_map = defaultdict(set)\n",
    "    for _, row in df_all.iterrows():\n",
    "        img_id = str(row[\"image_id\"])\n",
    "        for ent in [row[\"subject\"], row[\"object\"]]:\n",
    "            if ent in entity2id:\n",
    "                img_entity_map[img_id].add(entity2id[ent])\n",
    "    return img_entity_map\n",
    "\n",
    "# Đánh giá 1 triplet\n",
    "def evaluate_single_triplet_transH(head_id, rel_id, tail_id, entity_embeddings, relation_embeddings, image_embeddings, image_ids, img_entity_map, top_k=5):\n",
    "    pred_vec = entity_embeddings[head_id] + relation_embeddings[rel_id]\n",
    "    scores = []\n",
    "\n",
    "    for img_idx, img_id in enumerate(image_ids):\n",
    "        entities_in_img = img_entity_map.get(img_id, set())\n",
    "        if not entities_in_img:\n",
    "            continue\n",
    "        dists = [np.linalg.norm(pred_vec - entity_embeddings[eid]) for eid in entities_in_img]\n",
    "        score = min(dists)\n",
    "        scores.append((img_id, score))\n",
    "\n",
    "    scores = sorted(scores, key=lambda x: x[1])[:top_k]\n",
    "    return set([img_id for img_id, _ in scores])\n",
    "\n",
    "# Hàm đánh giá toàn bộ\n",
    "def evaluate_transH_model(entity_embeddings, relation_embeddings, image_embeddings, image_ids, test_triplets, entity2id, relation2id, img_entity_map, all_map, top_k=5):\n",
    "    gt_dict = defaultdict(set)\n",
    "    for _, row in all_map.iterrows():\n",
    "        key = (row[\"subject\"], row[\"predicate\"], row[\"object\"])\n",
    "        gt_dict[key].add(str(row[\"image_id\"]))\n",
    "\n",
    "    total_p, total_r, total_f1 = 0, 0, 0\n",
    "    count = 0\n",
    "\n",
    "    for _, row in tqdm(test_triplets.iterrows(), total=len(test_triplets)):\n",
    "        query_key = (row[\"subject\"], row[\"predicate\"], row[\"object\"])\n",
    "        h = entity2id.get(row[\"subject\"])\n",
    "        r = relation2id.get(row[\"predicate\"])\n",
    "        t = entity2id.get(row[\"object\"])\n",
    "        if h is None or r is None or t is None:\n",
    "            continue\n",
    "\n",
    "        pred_ids = evaluate_single_triplet_transH(h, r, t, entity_embeddings, relation_embeddings, image_embeddings, image_ids, img_entity_map, top_k)\n",
    "        true_ids = gt_dict.get(query_key, set())\n",
    "\n",
    "        if not true_ids:\n",
    "            continue\n",
    "\n",
    "        tp = len(pred_ids & true_ids)\n",
    "        fp = len(pred_ids - true_ids)\n",
    "        fn = len(true_ids - pred_ids)\n",
    "\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall    = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1        = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "        total_p += precision\n",
    "        total_r += recall\n",
    "        total_f1 += f1\n",
    "        count += 1\n",
    "\n",
    "    if count > 0:\n",
    "        print(f\"\\nEvaluation Results (top-{top_k}):\")\n",
    "        print(f\"→ Precision@{top_k}: {total_p/count:.4f}\")\n",
    "        print(f\"→ Recall@{top_k}:    {total_r/count:.4f}\")\n",
    "        print(f\"→ F1-score@{top_k}:  {total_f1/count:.4f}\")\n",
    "    else:\n",
    "        print(\"Không có triplet hợp lệ để đánh giá.\")\n",
    "\n",
    "# ========== CHẠY ==========\n",
    "if __name__ == \"__main__\":\n",
    "    # Đường dẫn\n",
    "    ENTITY_CSV = \"entity_embeddings_transh.csv\"\n",
    "    REL_CSV = \"relation_embeddings_transh.csv\"\n",
    "    IMAGE_FEAT = \"image_features.npy\"\n",
    "    IMAGE_IDS = \"image_ids.npy\"\n",
    "    TRIPLET_FILE = \"f_coco_triplets.csv\"\n",
    "\n",
    "    # Tải dữ liệu\n",
    "    entity_embeddings, relation_embeddings, entity2id, relation2id = load_embeddings(ENTITY_CSV, REL_CSV)\n",
    "    image_embeddings, image_ids = load_image_embeddings(IMAGE_FEAT, IMAGE_IDS)\n",
    "    df_all = pd.read_csv(TRIPLET_FILE)\n",
    "    img_entity_map = build_img_entity_map(df_all, entity2id)\n",
    "\n",
    "    # Tách tập test\n",
    "    df_test = df_all.sample(frac=0.2, random_state=42)  # hoặc dùng tập riêng nếu có\n",
    "\n",
    "    # Đánh giá\n",
    "    evaluate_transH_model(\n",
    "        entity_embeddings=entity_embeddings,\n",
    "        relation_embeddings=relation_embeddings,\n",
    "        image_embeddings=image_embeddings,\n",
    "        image_ids=image_ids,\n",
    "        test_triplets=df_test,\n",
    "        entity2id=entity2id,\n",
    "        relation2id=relation2id,\n",
    "        img_entity_map=img_entity_map,\n",
    "        all_map=df_all,\n",
    "        top_k=5\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0b7350",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
