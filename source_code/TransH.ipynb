{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11665244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 6.9233\n",
      "Epoch 2: Loss = 6.7290\n",
      "Epoch 3: Loss = 6.5307\n",
      "Epoch 4: Loss = 6.3209\n",
      "Epoch 5: Loss = 6.1294\n",
      "Epoch 6: Loss = 5.9450\n",
      "Epoch 7: Loss = 5.7262\n",
      "Epoch 8: Loss = 5.4979\n",
      "Epoch 9: Loss = 5.2914\n",
      "Epoch 10: Loss = 5.0357\n",
      "Epoch 11: Loss = 4.8294\n",
      "Epoch 12: Loss = 4.6041\n",
      "Epoch 13: Loss = 4.3904\n",
      "Epoch 14: Loss = 4.1842\n",
      "Epoch 15: Loss = 4.0101\n",
      "Epoch 16: Loss = 3.8447\n",
      "Epoch 17: Loss = 3.7278\n",
      "Epoch 18: Loss = 3.5192\n",
      "Epoch 19: Loss = 3.4311\n",
      "Epoch 20: Loss = 3.3440\n",
      "Epoch 21: Loss = 3.1814\n",
      "Epoch 22: Loss = 3.0008\n",
      "Epoch 23: Loss = 2.9855\n",
      "Epoch 24: Loss = 2.9706\n",
      "Epoch 25: Loss = 2.8416\n",
      "Epoch 26: Loss = 2.7769\n",
      "Epoch 27: Loss = 2.7230\n",
      "Epoch 28: Loss = 2.6188\n",
      "Epoch 29: Loss = 2.5830\n",
      "Epoch 30: Loss = 2.5822\n",
      "Epoch 31: Loss = 2.5454\n",
      "Epoch 32: Loss = 2.4507\n",
      "Epoch 33: Loss = 2.4049\n",
      "Epoch 34: Loss = 2.3026\n",
      "Epoch 35: Loss = 2.3853\n",
      "Epoch 36: Loss = 2.3329\n",
      "Epoch 37: Loss = 2.2391\n",
      "Epoch 38: Loss = 2.2497\n",
      "Epoch 39: Loss = 2.2175\n",
      "Epoch 40: Loss = 2.1963\n",
      "Epoch 41: Loss = 2.2250\n",
      "Epoch 42: Loss = 2.1372\n",
      "Epoch 43: Loss = 2.0784\n",
      "Epoch 44: Loss = 2.0373\n",
      "Epoch 45: Loss = 2.0965\n",
      "Epoch 46: Loss = 2.0263\n",
      "Epoch 47: Loss = 1.9690\n",
      "Epoch 48: Loss = 1.9822\n",
      "Epoch 49: Loss = 1.8922\n",
      "Epoch 50: Loss = 2.0065\n",
      "Epoch 51: Loss = 1.8951\n",
      "Epoch 52: Loss = 1.8970\n",
      "Epoch 53: Loss = 1.8516\n",
      "Epoch 54: Loss = 1.8042\n",
      "Epoch 55: Loss = 1.8409\n",
      "Epoch 56: Loss = 1.7773\n",
      "Epoch 57: Loss = 1.7720\n",
      "Epoch 58: Loss = 1.6670\n",
      "Epoch 59: Loss = 1.7685\n",
      "Epoch 60: Loss = 1.6960\n",
      "Epoch 61: Loss = 1.7284\n",
      "Epoch 62: Loss = 1.6975\n",
      "Epoch 63: Loss = 1.6621\n",
      "Epoch 64: Loss = 1.6562\n",
      "Epoch 65: Loss = 1.6256\n",
      "Epoch 66: Loss = 1.6143\n",
      "Epoch 67: Loss = 1.5760\n",
      "Epoch 68: Loss = 1.5641\n",
      "Epoch 69: Loss = 1.5192\n",
      "Epoch 70: Loss = 1.6268\n",
      "Epoch 71: Loss = 1.5243\n",
      "Epoch 72: Loss = 1.4900\n",
      "Epoch 73: Loss = 1.4662\n",
      "Epoch 74: Loss = 1.4460\n",
      "Epoch 75: Loss = 1.4446\n",
      "Epoch 76: Loss = 1.4780\n",
      "Epoch 77: Loss = 1.4654\n",
      "Epoch 78: Loss = 1.4453\n",
      "Epoch 79: Loss = 1.3712\n",
      "Epoch 80: Loss = 1.4345\n",
      "Epoch 81: Loss = 1.3933\n",
      "Epoch 82: Loss = 1.3621\n",
      "Epoch 83: Loss = 1.3723\n",
      "Epoch 84: Loss = 1.2800\n",
      "Epoch 85: Loss = 1.3703\n",
      "Epoch 86: Loss = 1.2922\n",
      "Epoch 87: Loss = 1.2544\n",
      "Epoch 88: Loss = 1.2931\n",
      "Epoch 89: Loss = 1.3157\n",
      "Epoch 90: Loss = 1.2586\n",
      "Epoch 91: Loss = 1.2715\n",
      "Epoch 92: Loss = 1.2118\n",
      "Epoch 93: Loss = 1.2386\n",
      "Epoch 94: Loss = 1.3086\n",
      "Epoch 95: Loss = 1.2161\n",
      "Epoch 96: Loss = 1.1996\n",
      "Epoch 97: Loss = 1.2023\n",
      "Epoch 98: Loss = 1.2448\n",
      "Epoch 99: Loss = 1.1735\n",
      "Epoch 100: Loss = 1.1957\n",
      "Epoch 101: Loss = 1.1616\n",
      "Epoch 102: Loss = 1.1919\n",
      "Epoch 103: Loss = 1.1625\n",
      "Epoch 104: Loss = 1.1457\n",
      "Epoch 105: Loss = 1.0931\n",
      "Epoch 106: Loss = 1.1297\n",
      "Epoch 107: Loss = 1.0950\n",
      "Epoch 108: Loss = 1.1119\n",
      "Epoch 109: Loss = 1.0935\n",
      "Epoch 110: Loss = 1.1130\n",
      "Epoch 111: Loss = 1.0799\n",
      "Epoch 112: Loss = 0.9874\n",
      "Epoch 113: Loss = 1.0562\n",
      "Epoch 114: Loss = 1.0729\n",
      "Epoch 115: Loss = 1.0487\n",
      "Epoch 116: Loss = 0.9585\n",
      "Epoch 117: Loss = 1.0083\n",
      "Epoch 118: Loss = 0.9743\n",
      "Epoch 119: Loss = 0.9856\n",
      "Epoch 120: Loss = 0.9818\n",
      "Epoch 121: Loss = 1.0056\n",
      "Epoch 122: Loss = 0.9402\n",
      "Epoch 123: Loss = 0.9221\n",
      "Epoch 124: Loss = 0.9382\n",
      "Epoch 125: Loss = 0.9998\n",
      "Epoch 126: Loss = 0.8766\n",
      "Epoch 127: Loss = 0.9539\n",
      "Epoch 128: Loss = 0.9090\n",
      "Epoch 129: Loss = 0.9458\n",
      "Epoch 130: Loss = 0.9455\n",
      "Epoch 131: Loss = 0.8686\n",
      "Epoch 132: Loss = 0.8817\n",
      "Epoch 133: Loss = 0.9289\n",
      "Epoch 134: Loss = 0.9109\n",
      "Epoch 135: Loss = 0.8996\n",
      "Epoch 136: Loss = 0.8503\n",
      "Epoch 137: Loss = 0.8206\n",
      "Epoch 138: Loss = 0.8515\n",
      "Epoch 139: Loss = 0.8410\n",
      "Epoch 140: Loss = 0.7816\n",
      "Epoch 141: Loss = 0.8459\n",
      "Epoch 142: Loss = 0.8323\n",
      "Epoch 143: Loss = 0.7880\n",
      "Epoch 144: Loss = 0.8172\n",
      "Epoch 145: Loss = 0.8006\n",
      "Epoch 146: Loss = 0.8306\n",
      "Epoch 147: Loss = 0.8378\n",
      "Epoch 148: Loss = 0.8333\n",
      "Epoch 149: Loss = 0.8614\n",
      "Epoch 150: Loss = 0.8060\n",
      "Epoch 151: Loss = 0.7690\n",
      "Epoch 152: Loss = 0.7857\n",
      "Epoch 153: Loss = 0.7964\n",
      "Epoch 154: Loss = 0.7482\n",
      "Epoch 155: Loss = 0.6988\n",
      "Epoch 156: Loss = 0.7548\n",
      "Epoch 157: Loss = 0.7668\n",
      "Epoch 158: Loss = 0.7390\n",
      "Epoch 159: Loss = 0.7696\n",
      "Epoch 160: Loss = 0.7473\n",
      "Epoch 161: Loss = 0.7629\n",
      "Epoch 162: Loss = 0.7138\n",
      "Epoch 163: Loss = 0.7194\n",
      "Epoch 164: Loss = 0.7526\n",
      "Epoch 165: Loss = 0.6713\n",
      "Epoch 166: Loss = 0.7341\n",
      "Epoch 167: Loss = 0.6713\n",
      "Epoch 168: Loss = 0.7304\n",
      "Epoch 169: Loss = 0.6922\n",
      "Epoch 170: Loss = 0.6733\n",
      "Epoch 171: Loss = 0.6586\n",
      "Epoch 172: Loss = 0.6818\n",
      "Epoch 173: Loss = 0.6447\n",
      "Epoch 174: Loss = 0.6146\n",
      "Epoch 175: Loss = 0.6609\n",
      "Epoch 176: Loss = 0.6835\n",
      "Epoch 177: Loss = 0.6354\n",
      "Epoch 178: Loss = 0.6759\n",
      "Epoch 179: Loss = 0.6462\n",
      "Epoch 180: Loss = 0.6306\n",
      "Epoch 181: Loss = 0.5975\n",
      "Epoch 182: Loss = 0.6308\n",
      "Epoch 183: Loss = 0.6148\n",
      "Epoch 184: Loss = 0.6471\n",
      "Epoch 185: Loss = 0.6199\n",
      "Epoch 186: Loss = 0.6077\n",
      "Epoch 187: Loss = 0.6395\n",
      "Epoch 188: Loss = 0.5746\n",
      "Epoch 189: Loss = 0.6175\n",
      "Epoch 190: Loss = 0.6122\n",
      "Epoch 191: Loss = 0.6316\n",
      "Epoch 192: Loss = 0.6147\n",
      "Epoch 193: Loss = 0.6169\n",
      "Epoch 194: Loss = 0.5780\n",
      "Epoch 195: Loss = 0.5282\n",
      "Epoch 196: Loss = 0.5035\n",
      "Epoch 197: Loss = 0.6029\n",
      "Epoch 198: Loss = 0.5929\n",
      "Epoch 199: Loss = 0.5910\n",
      "Epoch 200: Loss = 0.5923\n",
      "Epoch 201: Loss = 0.5753\n",
      "Epoch 202: Loss = 0.5484\n",
      "Epoch 203: Loss = 0.5578\n",
      "Epoch 204: Loss = 0.5117\n",
      "Epoch 205: Loss = 0.5401\n",
      "Epoch 206: Loss = 0.5643\n",
      "Epoch 207: Loss = 0.5715\n",
      "Epoch 208: Loss = 0.4970\n",
      "Epoch 209: Loss = 0.5062\n",
      "Epoch 210: Loss = 0.5312\n",
      "Epoch 211: Loss = 0.5105\n",
      "Epoch 212: Loss = 0.5194\n",
      "Epoch 213: Loss = 0.4836\n",
      "Epoch 214: Loss = 0.5171\n",
      "Epoch 215: Loss = 0.5270\n",
      "Epoch 216: Loss = 0.5486\n",
      "Epoch 217: Loss = 0.5089\n",
      "Epoch 218: Loss = 0.5215\n",
      "Epoch 219: Loss = 0.4705\n",
      "Epoch 220: Loss = 0.4644\n",
      "Epoch 221: Loss = 0.4570\n",
      "Epoch 222: Loss = 0.5141\n",
      "Epoch 223: Loss = 0.4771\n",
      "Epoch 224: Loss = 0.5069\n",
      "Epoch 225: Loss = 0.5086\n",
      "Epoch 226: Loss = 0.5064\n",
      "Epoch 227: Loss = 0.5019\n",
      "Epoch 228: Loss = 0.5029\n",
      "Epoch 229: Loss = 0.4814\n",
      "Epoch 230: Loss = 0.4537\n",
      "Epoch 231: Loss = 0.4482\n",
      "Epoch 232: Loss = 0.5215\n",
      "Epoch 233: Loss = 0.4601\n",
      "Epoch 234: Loss = 0.4510\n",
      "Epoch 235: Loss = 0.4745\n",
      "Epoch 236: Loss = 0.4883\n",
      "Epoch 237: Loss = 0.5113\n",
      "Epoch 238: Loss = 0.4827\n",
      "Epoch 239: Loss = 0.4793\n",
      "Epoch 240: Loss = 0.4422\n",
      "Epoch 241: Loss = 0.4459\n",
      "Epoch 242: Loss = 0.4499\n",
      "Epoch 243: Loss = 0.4283\n",
      "Epoch 244: Loss = 0.4435\n",
      "Epoch 245: Loss = 0.4383\n",
      "Epoch 246: Loss = 0.4483\n",
      "Epoch 247: Loss = 0.4554\n",
      "Epoch 248: Loss = 0.4422\n",
      "Epoch 249: Loss = 0.4543\n",
      "Epoch 250: Loss = 0.4297\n",
      "Epoch 251: Loss = 0.4168\n",
      "Epoch 252: Loss = 0.4387\n",
      "Epoch 253: Loss = 0.3973\n",
      "Epoch 254: Loss = 0.4378\n",
      "Epoch 255: Loss = 0.4210\n",
      "Epoch 256: Loss = 0.4156\n",
      "Epoch 257: Loss = 0.4270\n",
      "Epoch 258: Loss = 0.4140\n",
      "Epoch 259: Loss = 0.4058\n",
      "Epoch 260: Loss = 0.3920\n",
      "Epoch 261: Loss = 0.3924\n",
      "Epoch 262: Loss = 0.4048\n",
      "Epoch 263: Loss = 0.3781\n",
      "Epoch 264: Loss = 0.4342\n",
      "Epoch 265: Loss = 0.4378\n",
      "Epoch 266: Loss = 0.4378\n",
      "Epoch 267: Loss = 0.4100\n",
      "Epoch 268: Loss = 0.4019\n",
      "Epoch 269: Loss = 0.4095\n",
      "Epoch 270: Loss = 0.4392\n",
      "Epoch 271: Loss = 0.4061\n",
      "Epoch 272: Loss = 0.3719\n",
      "Epoch 273: Loss = 0.3459\n",
      "Epoch 274: Loss = 0.3948\n",
      "Epoch 275: Loss = 0.3840\n",
      "Epoch 276: Loss = 0.4124\n",
      "Epoch 277: Loss = 0.3783\n",
      "Epoch 278: Loss = 0.3979\n",
      "Epoch 279: Loss = 0.3673\n",
      "Epoch 280: Loss = 0.3893\n",
      "Epoch 281: Loss = 0.4294\n",
      "Epoch 282: Loss = 0.3815\n",
      "Epoch 283: Loss = 0.3690\n",
      "Epoch 284: Loss = 0.3777\n",
      "Epoch 285: Loss = 0.4074\n",
      "Epoch 286: Loss = 0.4127\n",
      "Epoch 287: Loss = 0.3922\n",
      "Epoch 288: Loss = 0.3793\n",
      "Epoch 289: Loss = 0.3995\n",
      "Epoch 290: Loss = 0.3675\n",
      "Epoch 291: Loss = 0.3681\n",
      "Epoch 292: Loss = 0.3657\n",
      "Epoch 293: Loss = 0.3656\n",
      "Epoch 294: Loss = 0.3638\n",
      "Epoch 295: Loss = 0.3780\n",
      "Epoch 296: Loss = 0.3632\n",
      "Epoch 297: Loss = 0.3762\n",
      "Epoch 298: Loss = 0.3532\n",
      "Epoch 299: Loss = 0.3823\n",
      "Epoch 300: Loss = 0.3846\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# Step 1: Đọc triplet và tạo ánh xạ\n",
    "df_triplet = pd.read_csv(\"f_coco_triplets.csv\")\n",
    "entities = pd.concat([df_triplet[\"subject\"], df_triplet[\"object\"]]).unique()\n",
    "entity2id = {entity: idx for idx, entity in enumerate(entities)}\n",
    "relation2id = {rel: idx for idx, rel in enumerate(df_triplet[\"predicate\"].unique())}\n",
    "\n",
    "# Lưu ánh xạ\n",
    "with open(\"entity2id.pkl\", \"wb\") as f:\n",
    "    pickle.dump(entity2id, f)\n",
    "with open(\"relation2id.pkl\", \"wb\") as f:\n",
    "    pickle.dump(relation2id, f)\n",
    "\n",
    "# Step 2: Gán ID cho triplets\n",
    "df_triplet[\"head_id\"] = df_triplet[\"subject\"].map(entity2id)\n",
    "df_triplet[\"tail_id\"] = df_triplet[\"object\"].map(entity2id)\n",
    "df_triplet[\"rel_id\"] = df_triplet[\"predicate\"].map(relation2id)\n",
    "triplets = df_triplet[[\"head_id\", \"rel_id\", \"tail_id\"]].values\n",
    "\n",
    "# Step 3: Định nghĩa mô hình TransH\n",
    "class TransH(nn.Module):\n",
    "    def __init__(self, num_entities, num_relations, embedding_dim=100, margin=1.0):\n",
    "        super().__init__()\n",
    "        self.margin = margin\n",
    "        self.ent_embeddings = nn.Embedding(num_entities, embedding_dim)\n",
    "        self.rel_embeddings = nn.Embedding(num_relations, embedding_dim)\n",
    "        self.norm_embeddings = nn.Embedding(num_relations, embedding_dim)\n",
    "        nn.init.xavier_uniform_(self.ent_embeddings.weight)\n",
    "        nn.init.xavier_uniform_(self.rel_embeddings.weight)\n",
    "        nn.init.xavier_uniform_(self.norm_embeddings.weight)\n",
    "\n",
    "    def _project(self, e, norm):\n",
    "        norm = F.normalize(norm, p=2, dim=-1)\n",
    "        return e - torch.sum(e * norm, dim=-1, keepdim=True) * norm\n",
    "\n",
    "    def forward(self, pos_triples, neg_triples):\n",
    "        ph = self.ent_embeddings(pos_triples[:, 0])\n",
    "        pr = self.rel_embeddings(pos_triples[:, 1])\n",
    "        pt = self.ent_embeddings(pos_triples[:, 2])\n",
    "        pn = self.norm_embeddings(pos_triples[:, 1])\n",
    "\n",
    "        nh = self.ent_embeddings(neg_triples[:, 0])\n",
    "        nr = self.rel_embeddings(neg_triples[:, 1])\n",
    "        nt = self.ent_embeddings(neg_triples[:, 2])\n",
    "        nn_ = self.norm_embeddings(neg_triples[:, 1])\n",
    "\n",
    "        ph_proj = self._project(ph, pn)\n",
    "        pt_proj = self._project(pt, pn)\n",
    "        nh_proj = self._project(nh, nn_)\n",
    "        nt_proj = self._project(nt, nn_)\n",
    "\n",
    "        pos_score = torch.norm(ph_proj + pr - pt_proj, p=2, dim=1)\n",
    "        neg_score = torch.norm(nh_proj + nr - nt_proj, p=2, dim=1)\n",
    "\n",
    "        return pos_score, neg_score\n",
    "\n",
    "# Step 4: Huấn luyện mô hình\n",
    "def corrupt_batch(batch, num_entities):\n",
    "    corrupted = batch.copy()\n",
    "    for i in range(len(batch)):\n",
    "        if np.random.rand() < 0.5:\n",
    "            corrupted[i][0] = np.random.randint(0, num_entities)\n",
    "        else:\n",
    "            corrupted[i][2] = np.random.randint(0, num_entities)\n",
    "    return corrupted\n",
    "\n",
    "def train_model(model, train_data, num_entities, batch_size=512, lr=0.001, epochs=300):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MarginRankingLoss(margin=model.margin)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        np.random.shuffle(train_data)\n",
    "        total_loss = 0\n",
    "\n",
    "        for i in range(0, len(train_data), batch_size):\n",
    "            pos_batch = train_data[i:i+batch_size]\n",
    "            neg_batch = corrupt_batch(pos_batch.copy(), num_entities)\n",
    "\n",
    "            pos_batch = torch.tensor(pos_batch, dtype=torch.long)\n",
    "            neg_batch = torch.tensor(neg_batch, dtype=torch.long)\n",
    "            y = torch.ones(pos_batch.size(0))\n",
    "\n",
    "            pos_score, neg_score = model(pos_batch, neg_batch)\n",
    "            loss = criterion(pos_score, neg_score, y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: Loss = {total_loss:.4f}\")\n",
    "\n",
    "    torch.save(model.state_dict(), \"transh_model.pth\")\n",
    "    return model\n",
    "\n",
    "model = TransH(num_entities=len(entity2id), num_relations=len(relation2id))\n",
    "train_model(model, triplets, num_entities=len(entity2id))\n",
    "\n",
    "# Step 5: Lưu embedding\n",
    "entity_emb = model.ent_embeddings.weight.detach().cpu().numpy()\n",
    "relation_emb = model.rel_embeddings.weight.detach().cpu().numpy()\n",
    "np.save(\"entity_embeddings.npy\", entity_emb)\n",
    "np.save(\"relation_embeddings.npy\", relation_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2942766c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 2613/2613 [00:00<00:00, 48009.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Precision@5: 0.0000\n",
      " Recall@5:    0.0000\n",
      " F1-score@5:  0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load lại ánh xạ\n",
    "with open(\"entity2id.pkl\", \"rb\") as f:\n",
    "    entity2id = pickle.load(f)\n",
    "with open(\"relation2id.pkl\", \"rb\") as f:\n",
    "    relation2id = pickle.load(f)\n",
    "\n",
    "# Load embeddings và ảnh\n",
    "entity_emb = np.load(\"entity_embeddings.npy\")\n",
    "relation_emb = np.load(\"relation_embeddings.npy\")\n",
    "image_emb = np.load(\"image_features.npy\")\n",
    "image_ids = np.load(\"image_ids.npy\")\n",
    "\n",
    "# Load triplet và ánh xạ ảnh → entity\n",
    "df = pd.read_csv(\"f_coco_triplets.csv\")\n",
    "img_entity_map = defaultdict(set)\n",
    "for _, row in df.iterrows():\n",
    "    img_id = str(row[\"image_id\"])\n",
    "    if row[\"subject\"] in entity2id:\n",
    "        img_entity_map[img_id].add(entity2id[row[\"subject\"]])\n",
    "    if row[\"object\"] in entity2id:\n",
    "        img_entity_map[img_id].add(entity2id[row[\"object\"]])\n",
    "\n",
    "# Đánh giá từng triplet\n",
    "def evaluate_single_triplet(h, r, t, top_k=5):\n",
    "    pred_vec = entity_emb[h] + relation_emb[r]\n",
    "    scores = []\n",
    "    for idx, img_id in enumerate(image_ids):\n",
    "        entities_in_img = img_entity_map.get(str(img_id), set())\n",
    "        if not entities_in_img:\n",
    "            continue\n",
    "        dists = [np.linalg.norm(pred_vec - entity_emb[eid]) for eid in entities_in_img]\n",
    "        scores.append((img_id, min(dists)))\n",
    "    top_k_imgs = sorted(scores, key=lambda x: x[1])[:top_k]\n",
    "    return set(str(img[0]) for img in top_k_imgs)\n",
    "\n",
    "# Tổng hợp GT để đánh giá\n",
    "gt_dict = defaultdict(set)\n",
    "for _, row in df.iterrows():\n",
    "    key = (row[\"subject\"], row[\"predicate\"], row[\"object\"])\n",
    "    gt_dict[key].add(str(row[\"image_id\"]))\n",
    "\n",
    "# Đánh giá toàn bộ test set\n",
    "total_p = total_r = total_f1 = 0\n",
    "count = 0\n",
    "for key in tqdm(gt_dict):\n",
    "    s, p, o = key\n",
    "    if s not in entity2id or o not in entity2id or p not in relation2id:\n",
    "        continue\n",
    "    h, r, t = entity2id[s], relation2id[p], entity2id[o]\n",
    "    pred = evaluate_single_triplet(h, r, t, top_k=5)\n",
    "    true = gt_dict[key]\n",
    "    if not true:\n",
    "        continue\n",
    "    tp = len(pred & true)\n",
    "    fp = len(pred - true)\n",
    "    fn = len(true - pred)\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    total_p += precision\n",
    "    total_r += recall\n",
    "    total_f1 += f1\n",
    "    count += 1\n",
    "\n",
    "print(f\"\\n Precision@5: {total_p/count:.4f}\")\n",
    "print(f\" Recall@5:    {total_r/count:.4f}\")\n",
    "print(f\" F1-score@5:  {total_f1/count:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ccc9ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
