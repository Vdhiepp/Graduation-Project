{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b6d3f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\envs\\coco_kg\\lib\\site-packages\\spacy\\util.py:837: UserWarning: [W095] Model 'en_core_web_sm' (3.5.0) was trained with spaCy v3.5 and may not be 100% compatible with the current version (3.3.3). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "2025-06-08 11:22:44.741 WARNING streamlit.runtime.caching.cache_data_api: No runtime found, using MemoryCacheStorageManager\n",
      "2025-06-08 11:22:44.753 WARNING streamlit.runtime.caching.cache_data_api: No runtime found, using MemoryCacheStorageManager\n",
      "2025-06-08 11:22:44.759 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-08 11:22:48.521 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run C:\\Users\\admin\\anaconda3\\envs\\coco_kg\\lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-06-08 11:22:48.521 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-08 11:22:48.529 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-08 11:22:48.532 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-08 11:22:48.533 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-08 11:22:48.534 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-08 11:22:48.535 Session state does not function when running a script without `streamlit run`\n",
      "2025-06-08 11:22:48.537 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-08 11:22:48.539 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import spacy\n",
    "import pickle\n",
    "import os\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "\n",
    "# Load mô hình NLP\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load encoder và ánh xạ từ model R-GCN\n",
    "with open(\"saved_model_R_GCN/entity_encoder.pkl\", \"rb\") as f:\n",
    "    entity_encoder = pickle.load(f)\n",
    "with open(\"saved_model_R_GCN/entity_idx_to_images.pkl\", \"rb\") as f:\n",
    "    entity_idx_to_images = pickle.load(f)\n",
    "with open(\"saved_model_R_GCN/synonym_map.pkl\", \"rb\") as f:\n",
    "    synonym_map = pickle.load(f)\n",
    "\n",
    "# Load thư mục ảnh\n",
    "image_folder = \"E:/Download/val2017\"\n",
    "\n",
    "# Hàm lấy cụm danh từ\n",
    "def get_full_noun_phrase(token):\n",
    "    mods = [child.text for child in token.children if child.dep_ in [\"amod\", \"compound\", \"det\", \"nummod\"]]\n",
    "    return \" \".join(mods + [token.text])\n",
    "\n",
    "# Hàm trích triplet\n",
    "@st.cache_data\n",
    "def extract_multiple_triplets(caption):\n",
    "    doc = nlp(caption)\n",
    "    triplets = []\n",
    "    subjects = set()\n",
    "    objects = set()\n",
    "    verb_subjects = {}\n",
    "    spatial_adverbs = {\n",
    "        \"outside\", \"inside\", \"nearby\", \"abroad\", \"indoors\", \"outdoors\", \"underground\",\n",
    "        \"overhead\", \"upstairs\", \"downstairs\", \"somewhere\", \"anywhere\", \"nowhere\",\n",
    "        \"back\", \"ahead\", \"overseas\", \"home\", \"away\"\n",
    "    }\n",
    "    for token in doc:\n",
    "        if token.dep_ in [\"nsubj\", \"nsubjpass\"]:\n",
    "            subj = token.text\n",
    "            verb_token = token.head\n",
    "            verb = verb_token.lemma_\n",
    "            verb_subjects[verb_token] = subj\n",
    "            subjects.add(subj)\n",
    "            for child in verb_token.children:\n",
    "                if child.dep_ in [\"dobj\", \"attr\"] and child.pos_ in [\"NOUN\", \"PROPN\"]:\n",
    "                    obj = get_full_noun_phrase(child)\n",
    "                    triplets.append((subj, verb, obj))\n",
    "                    objects.add(obj)\n",
    "                elif child.dep_ == \"prep\":\n",
    "                    for pobj in child.children:\n",
    "                        if pobj.dep_ == \"pobj\":\n",
    "                            obj = get_full_noun_phrase(pobj)\n",
    "                            triplets.append((subj, verb, obj))\n",
    "                            objects.add(obj)\n",
    "    return list(set(triplets))\n",
    "\n",
    "# Hàm tìm ảnh\n",
    "@st.cache_data\n",
    "def find_images_by_entities_prioritize_intersection(caption):\n",
    "    triplets = extract_multiple_triplets(caption)\n",
    "\n",
    "    def normalize(word):\n",
    "        if not word:\n",
    "            return None\n",
    "        word_lower = word.lower()\n",
    "        word_norm = synonym_map.get(word_lower, word_lower)\n",
    "        if word_norm in entity_encoder.classes_:\n",
    "            return word_norm\n",
    "        if word_norm.endswith(\"ing\"):\n",
    "            root = word_norm[:-3]\n",
    "            if root in entity_encoder.classes_:\n",
    "                return root\n",
    "        lemma = nlp(word_norm)[0].lemma_\n",
    "        if lemma in entity_encoder.classes_:\n",
    "            return lemma\n",
    "        return word_norm\n",
    "\n",
    "    def get_id(word):\n",
    "        try:\n",
    "            return entity_encoder.transform([word])[0]\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    image_counter = Counter()\n",
    "\n",
    "    for subj_raw, pred_raw, obj_raw in triplets:\n",
    "        subj = normalize(subj_raw)\n",
    "        pred = normalize(pred_raw)\n",
    "        obj = normalize(obj_raw)\n",
    "        subj_id = get_id(subj)\n",
    "        pred_id = get_id(pred)\n",
    "        obj_id = get_id(obj)\n",
    "        imgs = set()\n",
    "        if subj_id is not None and obj_id is not None:\n",
    "            subj_imgs = set(entity_idx_to_images.get(subj_id, []))\n",
    "            obj_imgs = set(entity_idx_to_images.get(obj_id, []))\n",
    "            core_imgs = subj_imgs & obj_imgs\n",
    "            if pred_id is not None:\n",
    "                pred_imgs = set(entity_idx_to_images.get(pred_id, []))\n",
    "                imgs = core_imgs & pred_imgs\n",
    "                if not imgs:\n",
    "                    imgs = subj_imgs | obj_imgs | pred_imgs\n",
    "            else:\n",
    "                imgs = core_imgs\n",
    "        elif subj_id is not None and pred_id is not None:\n",
    "            imgs = set(entity_idx_to_images.get(subj_id, [])) & set(entity_idx_to_images.get(pred_id, []))\n",
    "        elif obj_id is not None and pred_id is not None:\n",
    "            imgs = set(entity_idx_to_images.get(obj_id, [])) & set(entity_idx_to_images.get(pred_id, []))\n",
    "        elif subj_id is not None:\n",
    "            imgs = set(entity_idx_to_images.get(subj_id, []))\n",
    "        elif obj_id is not None:\n",
    "            imgs = set(entity_idx_to_images.get(obj_id, []))\n",
    "        elif pred_id is not None:\n",
    "            imgs = set(entity_idx_to_images.get(pred_id, []))\n",
    "        image_counter.update(imgs)\n",
    "\n",
    "    sorted_image_ids = [img_id for img_id, _ in image_counter.most_common()]\n",
    "    filenames = [f\"{int(img_id):012}.jpg\" for img_id in sorted_image_ids]\n",
    "    return filenames\n",
    "\n",
    "# === Giao diện Streamlit ===\n",
    "st.set_page_config(\n",
    "    page_title=\"Find images\",\n",
    "    layout=\"wide\",\n",
    "    initial_sidebar_state=\"auto\"\n",
    ")\n",
    "\n",
    "st.title(\"Find related images from the caption\")\n",
    "caption = st.text_input(\"Enter image description (caption):\")\n",
    "\n",
    "if caption:\n",
    "    filenames = find_images_by_entities_prioritize_intersection(caption)\n",
    "    if not filenames:\n",
    "        st.warning(\"No matching image found.\")\n",
    "    else:\n",
    "        st.success(f\"Found {len(filenames)} related images.\")\n",
    "        cols = st.columns(3)\n",
    "        for i, filename in enumerate(filenames[:9]):\n",
    "            col = cols[i % 3]\n",
    "            image_path = os.path.join(image_folder, filename)\n",
    "            if os.path.exists(image_path):\n",
    "                col.image(Image.open(image_path), caption=filename, use_column_width=True)\n",
    "            else:\n",
    "                col.write(f\"[Image file not found: {filename}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a4f39b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
